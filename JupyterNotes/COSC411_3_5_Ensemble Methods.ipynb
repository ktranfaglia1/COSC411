{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf800741",
   "metadata": {},
   "source": [
    "# COSC 411: Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eba9e",
   "metadata": {},
   "source": [
    "Instructor: Dr. Shuangquan (Peter) Wang\n",
    "\n",
    "Email: spwang@salisbury.edu\n",
    "\n",
    "Department of Computer Science, Salisbury University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c799d",
   "metadata": {},
   "source": [
    "# Module 3_ML algorithms and application\n",
    "\n",
    "## 5. Ensemble Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a29944",
   "metadata": {},
   "source": [
    "**Contents of this note refer to**\n",
    "1) https://colab.research.google.com/github/Eldave93/Seizure-Detection-Tutorials/blob/master/05.%20Ensemble%20Learning.ipynb#:~:text=Ensemble%20methods%20aim%20to%20improve,general%20methods%2C%20averaging%20and%20boosting\n",
    "2) Data Science Complete Tutorial. https://github.com/edyoda/data-science-complete-tutorial/blob/master/16.%20Ensemble%20Methods.ipynb\n",
    "3) Python toturial: https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6585bfe",
   "metadata": {},
   "source": [
    "**<font color=red>All rights reserved. Dissemination or sale of any part of this note is NOT permitted.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26e047",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "* Objective of ensemble methods is to combine the predictions of serveral base estimators ( Linear Regression, Decisison Tree, etc. ) to create a combined effect or more generalized model.\n",
    "* Two types of Ensemble Method\n",
    "  - Averaging Method : Build several estimators independently & average their predictions. Examples are RandomForest etc.\n",
    "  - Boosting Method : Base estimators are built sequentially using weighted version of data, i.e. fitting models with data that were mis-classified. Examples are AdaBoost\n",
    "  \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*PaXJ8HCYE9r2MgiZ32TQ2A.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4ca98",
   "metadata": {
    "id": "IEm1gMUbN3xO"
   },
   "source": [
    "## Bagging\n",
    "A bagging classifier is an ensemble of base classifiers, each fit on random subsets of a dataset. Their predictions are then pooled or aggregated to form a final prediction. This reduces variance of an estimator and can be a simple way to reduce overfitting. They work best with complex models as opposed to boosting, which work best with weak models<sup>1</sup>.\n",
    "\n",
    "Specifically, bagging is when sampling is produced with replacement<sup>2</sup>, and without replacement being called pasting<sup>3</sup>. Therefore both bagging and pasting allow training to be sampled several times across multipule predictors, with bagging only allowing several samples for the same predictor <sup>4</sup>.\n",
    "\n",
    "We can do this with any classifier so lets start with a support vector machine.\n",
    "\n",
    "**NOTE**\n",
    "- If we wanted to use pasting we would just set *bootstrap=False*.\n",
    "\n",
    "---\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/ensemble.html\n",
    "2. Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140.\n",
    "3. Breiman, L. (1999). Pasting small votes for classification in large databases and on-line. Machine learning, 36(1-2), 85-103.\n",
    "4. GÃ©ron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125222ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "lcsWLbOON8a5",
    "outputId": "bf41c0ff-ca4f-483e-a81f-64171c1021cd"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=0.8, random_state = 0)),\n",
    "                     ('clf', SVC(kernel='rbf', random_state=0))])\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=pipe_svc,\n",
    "                        n_estimators=10,\n",
    "                        max_samples=0.5,\n",
    "                        max_features=0.5,\n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=True,\n",
    "                        oob_score=True,\n",
    "                        warm_start=False,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b41c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_iris()\n",
    "#df = pd.DataFrame(dataset.data, columns=data.feature_names)\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442d40c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "d09pwIZFVbR2",
    "outputId": "bb8e9010-22d2-456f-b968-79f60803cf63"
   },
   "outputs": [],
   "source": [
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "display(pd.DataFrame(classification_report(y_test, y_pred , output_dict =True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd952e2",
   "metadata": {
    "id": "mGhcz25eVcqX"
   },
   "source": [
    "As we can see, performance is okay but recall is particularly poor. It is likely the model is not complex enough or models in the ensemble are too similar to each other (we'll look at solving this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6b335",
   "metadata": {
    "id": "UIYGfabe23JU"
   },
   "source": [
    "An additional way we can get a performance metric on a validation set is to ensure we use `oob_score = True`\n",
    "\n",
    "With bagging by default only trains on a sample of the training data, leaving a set of training data sampled as out-of-bag (oob) instances. Since they are not seen during training, we can evalute on them without a separate validation using the oob_score.\n",
    "\n",
    "It gets an accuracy of about 0.94 on the test/validation set, so it pretty close to what we did get above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6cd45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "iAhQUdYA28sk",
    "outputId": "8a95e355-f148-4bfb-c2a3-d36167f15169"
   },
   "outputs": [],
   "source": [
    "bag.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb72b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27468161",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "* Limitations of decison tree is that it overfits & shows high variance.\n",
    "* RandomForest is an averaging ensemble method whose prediction is function of prediction of 'n' decision trees.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Stavros_Dimitriadis/publication/324517994/figure/fig1/AS:615965951799303@1523869135381/Classification-process-based-on-the-Random-Forest-algorithm-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ae74",
   "metadata": {},
   "source": [
    "##### Algorithm\n",
    "* Data consist of R rows & M features.\n",
    "* Sample of training data is taken.\n",
    "* Random set of features are selected.\n",
    "* As many as configured number of trees are created using above two steps.\n",
    "* Final prediction in case of classification is majority prediction.\n",
    "* Final prediction in case of regression is mean/median of individual tree prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5633e",
   "metadata": {},
   "source": [
    "##### Comparing Decision Tree & Random Forest for MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95431560",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5461dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156a92d",
   "metadata": {},
   "source": [
    "##### Important Hyper-parameters\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* n_estimators : number of trees to be configured, larger is better but compute cost.\n",
    "* max_features : maximum number of features to be considered for splitting the node. For classification this equals to sqrt(n_features). And, for regression max_features = n_features.\n",
    "* n_jobs : Configure as -1 so that we can make use of all processors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aae673",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "* Minimal data cleaning or dealing with missing values required.\n",
    "* Works well with high dimensional datasets\n",
    "* Minimizes variance even for low variance models\n",
    "* RandomForest can tell importance of features. We can find important features & use them in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cef129",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8cea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "101c0bcd",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "* Boosting in general is about building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "\n",
    "##### Algorithm\n",
    "* Core concept of adaboost is to fit weak learners ( like decision tree ) sequantially on repeatedly modifying data.\n",
    "* Initially, each data is assigned equal weights.\n",
    "* A base estimator is fitted with this data.\n",
    "* Weights of misclassified data are increased & weights of correctly classified data is decreased. \n",
    "* Repeat the above two steps till all data are correctly classified or max number of iterations configured.\n",
    "* Making Prediction : The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dfaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8),n_estimators=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=20),n_estimators=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e197806",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc15804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
