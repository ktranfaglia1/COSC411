{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf800741",
   "metadata": {},
   "source": [
    "# COSC 411: Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eba9e",
   "metadata": {},
   "source": [
    "Instructor: Dr. Shuangquan (Peter) Wang\n",
    "\n",
    "Email: spwang@salisbury.edu\n",
    "\n",
    "Department of Computer Science, Salisbury University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c799d",
   "metadata": {},
   "source": [
    "# Module 3_ML algorithms and application\n",
    "\n",
    "## 2. Naive Bayes Classifiers (NBC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a29944",
   "metadata": {},
   "source": [
    "**Contents of this note refer to**\n",
    "1) \"Python Data Science Handbook\" Code Repository (https://github.com/jakevdp/PythonDataScienceHandbook); \n",
    "2) Python toturial: https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6585bfe",
   "metadata": {},
   "source": [
    "**<font color=red>All rights reserved. Dissemination or sale of any part of this note is NOT permitted.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0dc360",
   "metadata": {},
   "source": [
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
    "Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1841b",
   "metadata": {},
   "source": [
    "## Bayesian Classification\n",
    "\n",
    "Naive Bayes classifiers are built on Bayesian classification methods.\n",
    "These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
    "In Bayesian classification, we're interested in finding the probability of a label $L$ given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
    "$$\n",
    "\n",
    "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
    "$$\n",
    "\n",
    "All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d851f2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7eeb4",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.\n",
    "With this classifier, the assumption is that *data from each label is drawn from a simple Gaussian distribution*.\n",
    "Imagine that we have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5d469",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b75e0",
   "metadata": {},
   "source": [
    "The simplest Gaussian model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions.\n",
    "This model can be fit by computing the mean and standard deviation of the points within each label, which is all we need to define such a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176acfd6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5d95e",
   "metadata": {},
   "source": [
    "Let's generate some new data and predict the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e20a3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "ynew = model.predict(Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2864122",
   "metadata": {},
   "source": [
    "Now we can plot this new data to get an idea of where the decision boundary is (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b26924",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f699848",
   "metadata": {},
   "source": [
    "We see a slightly curved boundary in the classifications—in general, the boundary produced by a Gaussian naive Bayes model will be quadratic.\n",
    "\n",
    "A nice aspect is that it naturally allows for probabilistic classification, which we can compute using the `predict_proba` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c6bd9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "yprob = model.predict_proba(Xnew)\n",
    "yprob[-8:].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df04a93",
   "metadata": {},
   "source": [
    "The columns give the posterior probabilities of the first and second labels, respectively.\n",
    "If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a good place to start.\n",
    "\n",
    "Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results.\n",
    "Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a reliable method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb75423",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.\n",
    "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model it with a best-fit multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525452d",
   "metadata": {},
   "source": [
    "### Example: Classifying Text\n",
    "\n",
    "One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.\n",
    "Here we will use the sparse word count features from the 20 Newsgroups corpus made available through Scikit-Learn to show how we might classify these short documents into categories.\n",
    "\n",
    "Let's download the data and take a look at the target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235586f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adb096",
   "metadata": {},
   "source": [
    "For simplicity here, we will select just a few of these categories and download the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4704c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'rec.motorcycles',\n",
    "              'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a1496",
   "metadata": {},
   "source": [
    "In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.\n",
    "For this we will use the TF-IDF vectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), and create a pipeline that attaches it to a multinomial naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365dcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a84af",
   "metadata": {},
   "source": [
    "With this pipeline, we can apply the model to the training data and predict labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f7e54",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619aa3f",
   "metadata": {},
   "source": [
    "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.\n",
    "For example, let's take a look at the confusion matrix between the true and predicted labels for the test data (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ccbc8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names,\n",
    "            cmap='Blues')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbb432",
   "metadata": {},
   "source": [
    "We now have the tools to determine the category for *any* string, using the `predict` method of this pipeline.\n",
    "Here's a utility function that will return the prediction for a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6108f9d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a3688",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3863eb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predict_category('sending a payload to the ISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e98611",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predict_category('determining the screen resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf506a4",
   "metadata": {},
   "source": [
    "## When to Use Naive Bayes\n",
    "\n",
    "Because naive Bayes classifiers make such stringent assumptions about data, they will generally not perform as well as more complicated models.\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are fast for both training and prediction.\n",
    "- They provide straightforward probabilistic prediction.\n",
    "- They are often easily interpretable.\n",
    "- They have few (if any) tunable parameters.\n",
    "\n",
    "These advantages mean a naive Bayes classifier is often a good choice as an initial baseline classification.\n",
    "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.\n",
    "If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "Naive Bayes classifiers tend to perform especially well in the following situations:\n",
    "\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well-separated categories, when model complexity is less important\n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecb6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
